박해선 작가님. 구글클라우드이노베이터, 마이크로소프트 MVP

LLM. 대규모 언어모델.
오픈소스 LLM의 내부 구조를 코드 없이 간단하게 이해해보자.

# 트랜스포머 모델

입력 -> 인코더(압축), 한 번에 입력 -> 디코더(출력 생성) 과정, word 단위로 처리, 반복 -> 출력

GeMMa, LLaMMa 등은 인코더 없이 디코더만 사용하는 모델. 대부분이 그러함.

초기 텍스트(프롬프트)를 사람이 직접 집어넣어줘야 함.

모델이 클수록 실제 데이터를 처리하는 디코더 모듈의 개수가 증가함. 젬마2B 모델은 18개의 디코더를 가짐.

# 디코더 모듈

- 정규화 : 
	- 데이터의 scale을 일정 범위 내로 조절해주는 과정. 데이터 학습에 좋음. 다양한 방식이 있다.
	- 배치 정규화(인공신경망), 층 정규화(gpt2), RMS(root mean square, LLaMa)...
- 셀프 어텐션 메카니즘 : 데이터베이스의 쿼리 개념과 유사. 
	입력으로 들어온 데이터를 서로 다른 세 층을 경유시켜 query, key, value를 계산
	query와 key를 곱하고, 이를 다시 value와 곱해서 값을 얻음.
	이러한 과정을 셀프 어텐션, 어텐션 모듈을 여럿 가지기에 Multi-head attention이라 부름.
	어텐션 하나가 문맥을 학습하기에 어테션을 생성하는 헤드가 많으면 좋음.
	다만 어텐션각각을 모두 기억해야 해서 메모리 부하가 증가함.
- 정규화
- 피드 포워드 네트워크 : 되먹임 네트워크
	1. 밀집층 : 입력에 가중치 곱
	2. ReLU : 활성함수, 비선형, 두 입력 중 큰 값을 취하는 함수
	3. 밀집층
	4. 드랍아웃 : 학습한 가중치에서 일부를 확률적으로 삭제하는 부분. 과적합을 막아 일반성을 유지하는 방법. 반드시 확률적으로...
1. 

 